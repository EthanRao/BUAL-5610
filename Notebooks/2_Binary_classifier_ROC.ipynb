{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Evaluating Binary Classifiers\n",
    "\n",
    "In classification, our target column has a finite set of possible values which represent different categories a row can belong to. We use integers to represent the different categories so we can continue to use mathematical functions to describe how the independent variables map to the dependent variable. Here are a few examples of classification problems:\n",
    "\n",
    "Problem\t| Sample Features |\tType |\tCategories | Numerical Categories\n",
    "--- | --- | ---  | ---  | ---\n",
    "Should we accept this student based on their graduate school application? | ollege GPA, SAT Score, Quality of Recommendations |\tBinary |\tDon't Accept, Accept |\t0, 1\n",
    "What is the most likely blood type of 2 parent's offspring?\t| Parent 1's blood type, Parent 2's blood type.\t| Multi-class |\tA, B, AB, O\t | 1, 2, 3, 4\n",
    "\n",
    "\n",
    "\n",
    "We'll focus on binary classification for now, where the only 2 options for values are:\n",
    "\n",
    "0 for the False condition,\n",
    "1 for the True condition.\n",
    "\n",
    "Before we dive into classification, let's understand the data we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction To The Data\n",
    "\n",
    "We will begin by examining some numerical and graphical summaries of the **Smarket** data, which is part of the ISLR library for our textbook. \n",
    "\n",
    "This data set consists of percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, *Lag1* through *Lag5*. We have also recorded *Volume* (the number of shares traded on the previous day, in billions), *Today* (the percentage return on the date in question) and *Direction* (whether the market was Up or Down on this date). \n",
    "\n",
    "In this example, we will fit a logistic regression model in order to predict *Direction* using *Lag1* through *Lag5* and *Volume*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1250 entries, 0 to 1249\n",
      "Data columns (total 10 columns):\n",
      "Unnamed: 0    1250 non-null int64\n",
      "Year          1250 non-null int64\n",
      "Lag1          1250 non-null float64\n",
      "Lag2          1250 non-null float64\n",
      "Lag3          1250 non-null float64\n",
      "Lag4          1250 non-null float64\n",
      "Lag5          1250 non-null float64\n",
      "Volume        1250 non-null float64\n",
      "Today         1250 non-null float64\n",
      "Direction     1250 non-null object\n",
      "dtypes: float64(7), int64(2), object(1)\n",
      "memory usage: 97.7+ KB\n",
      "None\n",
      "(1250, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today  \\\n",
       "0           1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959   \n",
       "1           2  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032   \n",
       "2           3  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623   \n",
       "3           4  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614   \n",
       "4           5  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213   \n",
       "\n",
       "  Direction  \n",
       "0        Up  \n",
       "1        Up  \n",
       "2      Down  \n",
       "3        Up  \n",
       "4        Up  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read Smarket.csv into a Dataframe named stocks\n",
    "stocks = pd.read_csv('Data/Smarket.csv') \n",
    "\n",
    "# print the information of the dataset\n",
    "print(stocks.info())\n",
    "\n",
    "# print # of rows, # of columns\n",
    "print(stocks.shape)\n",
    "\n",
    "# print the first five rows\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Down</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today  \\\n",
       "0           1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959   \n",
       "1           2  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032   \n",
       "2           3  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623   \n",
       "3           4  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614   \n",
       "4           5  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213   \n",
       "\n",
       "  Direction  Down  Up  \n",
       "0        Up     0   1  \n",
       "1        Up     0   1  \n",
       "2      Down     1   0  \n",
       "3        Up     0   1  \n",
       "4        Up     0   1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert Direction to dummy variables\n",
    "\n",
    "stocks_up = pd.get_dummies(stocks['Direction'])\n",
    "\n",
    "# Join the dummy variables to the main dataframe\n",
    "stocks_new = pd.concat([stocks, stocks_up], axis=1)\n",
    "stocks_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "\n",
    "In the previous scatter plot, you'll notice that the *Lag1* column and the *Up* column do not have a clear linear relationship.\n",
    "\n",
    "Recall that the Up column only contains the values 0 and 1 and are used to represent binary values and the numbers themselves don't carry any weight. When numbers are used to represent different options or categories, they are referred to as **categorical** values. Classification focuses on estimating the relationship between the independent variables and the dependent, categorical variable.\n",
    "\n",
    "\n",
    "In this unit, we'll focus on a classification technique called **logistic regression**. While a linear regression model outputs a real number as the label, a logistic regression model outputs a *probability* value. In binary classification, if the probability value is larger than a certain **threshold probability**, we assign the label for that row to 1 or 0 otherwise.\n",
    "\n",
    "\n",
    "\n",
    "This threshold probability is something we select, and we'll learn about how to select a good threshold probability in later missions. For now, let's dive more into how logistic regression works.\n",
    "\n",
    "\n",
    "\n",
    "In linear regression, we used the linear function *y = mx + b* to represent the relationship between the independent variables and the dependent variable. In logistic regression, we use the logit function, which is a version of the linear function that is adapted for classification.\n",
    "\n",
    "\n",
    "\n",
    "Let's explore some of the logit function's properties to better understand why it's useful for classification tasks. Unlike in linear regression, where the output can be any real value, in logistic regression the output has to be a real value between 0 and 1, since the output represents a probability value. Note that the model can't output a negative value or it would violate this criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Training A Logistic Regression Model\n",
    "\n",
    "Let's now move onto training the logistic regression model using our dataset. \n",
    "\n",
    " We'll be using the scikit-learn library to fit a model between the lag variables and the Up columns. Training a logistic regression model in scikit-learn is similar to training a linear regression model, with the key difference that we use the LogisticRegression class instead of the LinearRegression class. Scikit-learn was designed to make it easy to swap out models by keeping the syntax and notation as consistent as possible across it's different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07279035, -0.04229102,  0.010958  ,  0.00921799,  0.01023031,\n",
       "         0.12793931]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the x columns and y variable\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression()\n",
    "x_columns = [\"Lag1\", \"Lag2\", \"Lag3\", \"Lag4\", \"Lag5\", \"Volume\"]\n",
    "\n",
    "# fit a logistic model\n",
    "logistic_model.fit(stocks_new[x_columns], stocks_new[\"Up\"])\n",
    "logistic_model.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predicted probability\n",
    "pred_probs = logistic_model.predict_proba(stocks_new[x_columns])\n",
    "\n",
    "# create a new column to store the probability\n",
    "stocks_new[\"Prediction\"] = pred_probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LogisticRegression method predict to return the predicted for each label in the training set.\n",
    "\n",
    "fitted_labels = logistic_model.predict(stocks_new[x_columns])\n",
    "stocks_new[\"predicted_label\"] = fitted_labels\n",
    "stocks_new[\"actual_label\"] = stocks_new[\"Up\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Outcomes\n",
    "\n",
    "It looks like the raw accuracy is around 52.5% which is slightly better than randomly guessing the label (which would result in around a 50% accuracy). Calculating the accuracy of a model on the dataset used for training is a useful initial step just to make sure the model at least beats randomly assigning a label for each observation. However, prediction accuracy doesn't tell us much more.\n",
    "\n",
    "The accuracy doesn't tell us how the model performs on data it wasn't trained on. A model that returns a 100% accuracy when evaluated on it's training set doesn't tell us how well the model works on data it's never seen before (and wasn't trained on). Accuracy also doesn't help us discriminate between the different types of outcomes a binary classification model can make. In a later mission, we'll learn how to evaluate a model's effectiveness on new, unseen data. In this mission, we'll focus on the principles of evaluating binary classification models by testing our model's effectiveness on the training data.\n",
    "\n",
    "To start, let's discuss the 4 different outcomes of a binary classification model in the following **confusion matrix**:\n",
    "\n",
    " | Predicted |  Predicted\n",
    "--- | ---  | --- \n",
    "**Observation** | Down (0) | Up (1)\n",
    "Down (0) | True Negative (TN) | False Positive (FP)\n",
    "Up (1) | False Negative (FN) | True Positive (TP)\n",
    "\n",
    "By segmenting a model's predictions into these different outcome categories, we can start to think about other measures of effectiveness that give us more granularity than simple accuracy.\n",
    "\n",
    "We can define these outcomes as:\n",
    "\n",
    "**True Postive** - The model correctly predicted that the market went up.\n",
    "\n",
    "Said another way, the model predicted that the label would be Positive, and that ended up being True.\n",
    "For this dataset, a true positive is whenever predicted_label is 1 and actual_label is 1.\n",
    "\n",
    "\n",
    "**True Negative** - The model correctly predicted that the market went down.\n",
    "\n",
    "Said another way, the model predicted that the label would be Negative, and that ended up being True.\n",
    "For this dataset, a true negative is whenever predicted_label is 0 and actual_label is 0.\n",
    "\n",
    "\n",
    "**False Positive** - The model incorrectly predicted that the market went up even though the market went down.\n",
    "\n",
    "Said another way, the model predicted that the label would be Positive, but that was False (the actual label was True).\n",
    "For this dataset, a false positive is whenever predicted_label is 1 but the actual_label is 0.\n",
    "\n",
    "**False Negative** - The model incorrectly predicted that the market went down even though the market went up.\n",
    "\n",
    "Said another way, the model predicted that the would be Negative, but that was False (the actual value was True).\n",
    "For this dataset, a false negative is whenever predicted_label is 0 but the actual_label is 1.\n",
    "\n",
    "\n",
    "Let's calculate the number of observations that fall into some of these outcome categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[143, 459],\n",
       "       [135, 513]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(stocks_new[\"actual_label\"], stocks_new[\"predicted_label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 459 135 513\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix\\\n",
    "(stocks_new[\"actual_label\"], stocks_new[\"predicted_label\"]).ravel() \n",
    "print(tn, fp, fn, tp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "The stocks_new Dataframe now contains the predicted value for that row, in the predicted_label column, and the actual value for that row, in the Up column. This format makes it easier for us to calculate how effective the model was on the training data. The simplest way to determine the effectiveness of a classification model is prediction accuracy. Accuracy helps us answer the question:\n",
    "\n",
    "** What fraction of the predictions were correct (actual label matched predicted label)? **\n",
    "\n",
    "Prediction accuracy boils down to the number of labels that were correctly predicted divided by the total number of observations:\n",
    "\n",
    "** Accuracy = # of Correctly Predicted / # of Observations **\n",
    "\n",
    "\n",
    "In logistic regression, recall that the model's output is a probability between 0 and 1. To decide whether the market was Up on that day, we set a threshold and mark those day as Up days when their computed probability exceeds that threshold. This threshold is called the ** discrimination threshold ** and scikit-learn sets it to 0.5 by default when predicting labels. If the predicted probability is greater than 0.5, the label for that observation is 1. If it is instead less than 0.5, the label for that observation is 0.\n",
    "\n",
    "An accuracy of 1.0 means that the model predicted 100% of days correctly for the given discrimination threshold. An accuracy of 0.2 means that the model predicted 20% of the days correctly. Let's calculate the accuracy for the predictions the logistic regression model made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity\n",
    "\n",
    "Let's now look at a few measures that are much more insightful than simple accuracy. Let's start with sensitivity:\n",
    "\n",
    "** Sensitivity or True Positive Rate **  - The proportion of days that actually went up\n",
    "\n",
    "\n",
    "** TPR = True Positives / (True Positives + False Negatives) ** \n",
    "\n",
    "Of all of the days that went up (True Positives + False Negatives), what fraction did the model correctly predict the movement (True Positives)? More generally, this measure helps us answer the question:\n",
    "\n",
    "** How effective is this model at identifying positive outcomes? ** \n",
    "\n",
    "In our case, the positive outcome (label of 1) is the market went up. If the True Positive Rate is low, it means that the model isn't effective at catching positive cases. \n",
    "\n",
    "For certain problems, high sensitivity is incredibly important. If we're building a model to predict which patients have cancer, every patient that is missed by the model could mean a loss of life. We want a highly sensitive model that is able to \"catch\" all of the positive cases (in this case, the positive case is a patient with cancer).\n",
    "\n",
    "Let's calculate the sensitivity for the model we're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "\n",
    "Looks like the sensitivity of the model is around 80% and about 8 of 10 up days were actually captured by our model. \n",
    "\n",
    "In the healthcare context, however, low sensitivity could mean a severe loss of life. If a classification model is only catching 12.7% of positive cases for an illness, then around 7 of 8 people are going undiagnosed (being classified as false negatives). Hopefully you're beginning to acquire a sense for the tradeoffs predictive models make and the importance of understanding the various measures.\n",
    "\n",
    "Let's now learn about ** specificity** :\n",
    "\n",
    "** Specificity or True Negative Rate**  - The proportion of applicants that were correctly rejected:\n",
    "\n",
    "** TNR = True Negatives / (False Positives + True Negatives)** \n",
    "\n",
    "\n",
    "This helps us answer the question:\n",
    "\n",
    "** How effective is this model at identifying negative outcomes?** \n",
    "\n",
    "A high specificity means that the model is really good at predicting which days went down.\n",
    "\n",
    "Let's calculate the specificity of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5248\n",
      "Sensitivy = 0.7916666666666666\n",
      "Specificity = 0.23754152823920266\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %s\" %((tn+tp)/(tn+tp+fn+fp)))\n",
    "print(\"Sensitivy = %s\" %(tp/(tp+fn)))\n",
    "print(\"Specificity = %s\" %(tn/(tn+fp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "We can vary the **discrimination threshold** and calculate the TPR and FPR for each value. This is called an ROC curve, which stands for reciever operator curve, and it allows us to understand a classification model's performance as the discrimination threshold is varied. To calculate the TPR and FPR values *at each discrimination threshold*, we can use the scikit-learn roc_curve function. This function will calculate the false positive rate and true positive rate for varying discrimination thresholds until both reach 0%.\n",
    "\n",
    "This function takes 2 required parameters:\n",
    "\n",
    "y_true: list of the true labels for the observations,\n",
    "y_score: list of the model's probability scores for those observations.\n",
    "As the example code in the documentation suggests, the roc_curve function returns 3 values which you can assign all at once:\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels, probabilities)\n",
    "\n",
    "You'll notice that the returned thresholds won't usually range from 0.0 to 1.0 and will instead constrains the result set to the minimum range where FPR and TPR range from 0.0 to 1.0. Once we have the FPR and TPR for each relevant threshold, we can plot the ROC curve using the Matplotlib plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b1dfd99e10>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(stocks_new[\"actual_label\"], stocks_new[\"Prediction\"]) # pred_probs[:,1]\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under The Curve\n",
    "\n",
    "When looking at an ROC curve, you want to keep an eye on how the 2 measures trade off and select an appropriate threshold based on your priorities. \n",
    "\n",
    "An investor benefits the most if the model does a wonderful job in recognizing ___________________ .\n",
    "\n",
    "We can now go one step further and determine the area under the curve or AUC for short. The AUC describes the probability that the classifier will rank a random positive observation higher than a random negative observation. Since randomly guessing converges to a probability of 0.5, the higher the AUC the more accurate the model seems to be.\n",
    "\n",
    "To calculate the AUC, we can use the scikit-learn function roc_auc_score, which takes the same parameters as the roc_curve function and returns a single float value corresponding to the AUC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5386315163446946\n"
     ]
    }
   ],
   "source": [
    "# Means we can just use roc_auc_curve() instead of metrics.roc_auc_curve()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = roc_auc_score(stocks_new[\"actual_label\"], stocks_new[\"Prediction\"])\n",
    "print(auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
